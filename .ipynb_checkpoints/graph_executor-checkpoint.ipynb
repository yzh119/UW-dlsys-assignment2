{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "- [x] python/dlsys/autodiff.py\n",
    "- [ ] src/gpu_op.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile python/dlsys/autodiff.py\n",
    "# %load python/dlsys/autodiff.py\n",
    "\"\"\" library to take autodiff and execute a computation graph \"\"\"\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import numpy as np\n",
    "from . import ndarray, gpu_op\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"Node in a computation graph.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Constructor, new node is indirectly created by Op object call method.\n",
    "\n",
    "            Instance variables\n",
    "            ------------------\n",
    "            self.inputs: the list of input nodes.\n",
    "            self.op: the associated op object,\n",
    "                e.g. add_op if this node is created by adding two other nodes.\n",
    "            self.const_attr: the add or multiply constant.\n",
    "                e.g. self.const_attr=5 if this node is created by x+5.\n",
    "            self.name: node name for debugging.\n",
    "        \"\"\"\n",
    "        self.inputs = []\n",
    "        self.op = None\n",
    "        self.const_attr = None\n",
    "        self.name = \"\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"Adding two nodes return a new node.\"\"\"\n",
    "        if isinstance(other, Node):\n",
    "            new_node = add_op(self, other)\n",
    "        else:\n",
    "            # Add by a constant stores the constant in new node's const_attr\n",
    "            # 'other' argument is a constant\n",
    "            new_node = add_byconst_op(self, other)\n",
    "        return new_node\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"Multiplying two nodes return a new node.\"\"\"\n",
    "        if isinstance(other, Node):\n",
    "            new_node = mul_op(self, other)\n",
    "        else:\n",
    "            # Mul by a constant stores the constant in new node's const_attr\n",
    "            # 'other' argument is a constant\n",
    "            new_node = mul_byconst_op(self, other)\n",
    "        return new_node\n",
    "\n",
    "    # Allow left-hand-side add and multiply.\n",
    "    __radd__ = __add__\n",
    "    __rmul__ = __mul__\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Allow print to display node name.\"\"\"\n",
    "        return self.name\n",
    "\n",
    "\n",
    "def Variable(name):\n",
    "    \"\"\"User defined variables in an expression.\n",
    "        e.g. x = Variable(name = \"x\")\n",
    "    \"\"\"\n",
    "    placeholder_node = placeholder_op()\n",
    "    placeholder_node.name = name\n",
    "    return placeholder_node\n",
    "\n",
    "\n",
    "class Op(object):\n",
    "    \"\"\"Op represents operations performed on nodes.\"\"\"\n",
    "    def __call__(self):\n",
    "        \"\"\"Create a new node and associate the op object with the node.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The new node object.\n",
    "        \"\"\"\n",
    "        new_node = Node()\n",
    "        new_node.op = self\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        \"\"\"Given values of input nodes, compute the output value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node: node that performs the compute.\n",
    "        input_vals: values of input nodes.\n",
    "        output_val: output value of the node, modified in-place.\n",
    "        use_numpy: bool flag whether to use numpy for compute\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        \"\"\"Given output gradient, compute partial gradient to each input node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node: node that performs the gradient.\n",
    "        output_grad: output gradient summed from children nodes' contributions\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A list of gradient contributions to each input node respectively.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        \"\"\"Given shapes of input nodes, compute shape of output node.\n",
    "\n",
    "        Implementation note:\n",
    "        It's simpler to treat shape of constants as (1,), so that constants can\n",
    "        be stored as a numpy array too and you would need fewer special case\n",
    "        handling.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node: node whose shape is being inferred.\n",
    "        input_vals: shapes of input nodes.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A tuple representing the shape of output node.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class AddOp(Op):\n",
    "    def __call__(self, node_A, node_B):\n",
    "        new_node = Op.__call__(self)\n",
    "        new_node.inputs = [node_A, node_B]\n",
    "        new_node.name = \"(%s+%s)\" % (node_A.name, node_B.name)\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        assert len(input_vals) == 2\n",
    "        if use_numpy:\n",
    "            # output_val[:] allows modify in-place\n",
    "            output_val[:] = input_vals[0] + input_vals[1]\n",
    "        else:\n",
    "            if input_vals[0].shape == input_vals[1].shape:\n",
    "                gpu_op.matrix_elementwise_add(\n",
    "                    input_vals[0], input_vals[1], output_val)\n",
    "            else:\n",
    "                if input_vals[1].shape == (1,):\n",
    "                    const_val = input_vals[1].asnumpy()[0]\n",
    "                    gpu_op.matrix_elementwise_add_by_const(\n",
    "                        input_vals[0], const_val, output_val)\n",
    "                elif input_vals[0].shape == (1,):\n",
    "                    const_val = input_vals[0].asnumpy()[0]\n",
    "                    gpu_op.matrix_elementwise_add_by_const(\n",
    "                        input_vals[1], const_val, output_val)\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        return [output_grad, output_grad]\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        \"\"\"Need to handle input_vals[0].shape != input_vals[1].shape\"\"\"\n",
    "        assert input_shapes[0] == input_shapes[1]\n",
    "        return input_shapes[0]\n",
    "\n",
    "\n",
    "class AddByConstOp(Op):\n",
    "    def __call__(self, node_A, const_val):\n",
    "        new_node = Op.__call__(self)\n",
    "        new_node.const_attr = const_val\n",
    "        new_node.inputs = [node_A]\n",
    "        new_node.name = \"(%s+%s)\" % (node_A.name, str(const_val))\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        assert len(input_vals) == 1\n",
    "        if use_numpy:\n",
    "            output_val[:] = input_vals[0] + node.const_attr\n",
    "        else:\n",
    "            gpu_op.matrix_elementwise_add_by_const(\n",
    "                input_vals[0], node.const_attr, output_val)\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        return [output_grad]\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        return input_shapes[0]\n",
    "\n",
    "\n",
    "class MulOp(Op):\n",
    "    def __call__(self, node_A, node_B):\n",
    "        new_node = Op.__call__(self)\n",
    "        new_node.inputs = [node_A, node_B]\n",
    "        new_node.name = \"(%s*%s)\" % (node_A.name, node_B.name)\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        assert len(input_vals) == 2\n",
    "        if use_numpy:\n",
    "            output_val[:] = input_vals[0] * input_vals[1]\n",
    "        else:\n",
    "            if input_vals[0].shape == input_vals[1].shape:\n",
    "                gpu_op.matrix_elementwise_multiply(\n",
    "                    input_vals[0], input_vals[1], output_val)\n",
    "            else:\n",
    "                if input_vals[1].shape == (1,):\n",
    "                    const_val = input_vals[1].asnumpy()[0]\n",
    "                    gpu_op.matrix_elementwise_multiply_by_const(\n",
    "                        input_vals[0], const_val, output_val)\n",
    "                elif input_vals[0].shape == (1,):\n",
    "                    const_val = input_vals[0].asnumpy()[0]\n",
    "                    gpu_op.matrix_elementwise_multiply_by_const(\n",
    "                        input_vals[1], const_val, output_val)\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        return [node.inputs[1] * output_grad, node.inputs[0] * output_grad]\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        \"\"\"Need to handle input_vals[0].shape != input_vals[1].shape\"\"\"\n",
    "        assert input_shapes[0] == input_shapes[1]\n",
    "        return input_shapes[0]\n",
    "        \n",
    "\n",
    "class MulByConstOp(Op):\n",
    "    def __call__(self, node_A, const_val):\n",
    "        new_node = Op.__call__(self)\n",
    "        new_node.const_attr = const_val\n",
    "        new_node.inputs = [node_A]\n",
    "        new_node.name = \"(%s*%s)\" % (node_A.name, str(const_val))\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        assert len(input_vals) == 1\n",
    "        if use_numpy:\n",
    "            output_val[:] = input_vals[0] * node.const_attr\n",
    "        else:\n",
    "            gpu_op.matrix_elementwise_multiply_by_const(\n",
    "                input_vals[0], node.const_attr, output_val)\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        return [node.const_attr * output_grad]\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        return input_shapes[0]\n",
    "\n",
    "\n",
    "class MatMulOp(Op):\n",
    "    def __call__(self, node_A, node_B, trans_A=False, trans_B=False):\n",
    "        new_node = Op.__call__(self)\n",
    "        new_node.matmul_attr_trans_A = trans_A\n",
    "        new_node.matmul_attr_trans_B = trans_B\n",
    "        new_node.inputs = [node_A, node_B]\n",
    "        new_node.name = \"MatMul(%s,%s,%s,%s)\" % (\n",
    "            node_A.name, node_B.name, str(trans_A), str(trans_B))\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        if use_numpy:\n",
    "            if ((node.matmul_attr_trans_A is False) and\n",
    "                    (node.matmul_attr_trans_B is False)):\n",
    "                output_val[:] = np.matmul(input_vals[0], input_vals[1])\n",
    "            elif ((node.matmul_attr_trans_A is True) and\n",
    "                    (node.matmul_attr_trans_B is False)):\n",
    "                output_val[:] = np.matmul(\n",
    "                    np.transpose(input_vals[0]), input_vals[1])\n",
    "            elif ((node.matmul_attr_trans_A is False) and\n",
    "                    (node.matmul_attr_trans_B is True)):\n",
    "                output_val[:] = np.matmul(\n",
    "                    input_vals[0], np.transpose(input_vals[1]))\n",
    "            elif ((node.matmul_attr_trans_A is True) and\n",
    "                    (node.matmul_attr_trans_B is True)):\n",
    "                output_val[:] = np.matmul(\n",
    "                    np.transpose(input_vals[0]), np.transpose(input_vals[1]))\n",
    "        else:\n",
    "            gpu_op.matrix_multiply(\n",
    "                input_vals[0], node.matmul_attr_trans_A,\n",
    "                input_vals[1], node.matmul_attr_trans_B,\n",
    "                output_val)\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        if ((node.matmul_attr_trans_A is False) and\n",
    "                (node.matmul_attr_trans_B is False)):\n",
    "            # if Y=AB, then dA=dY B^T, dB=A^T dY\n",
    "            lhs_grad = matmul_op(\n",
    "                output_grad, node.inputs[1], trans_A=False, trans_B=True)\n",
    "            rhs_grad = matmul_op(\n",
    "                node.inputs[0], output_grad, trans_A=True, trans_B=False)\n",
    "        elif ((node.matmul_attr_trans_A is True) and\n",
    "                (node.matmul_attr_trans_B is False)):\n",
    "            # if Y=A^T B, then dA=(dY B^T)^T=B dY^T, dB=A^T dY\n",
    "            lhs_grad = matmul_op(\n",
    "                node.inputs[1], output_grad, trans_A=False, trans_B=True)\n",
    "            rhs_grad = matmul_op(\n",
    "                node.inputs[0], output_grad, trans_A=True, trans_B=False)\n",
    "        elif ((node.matmul_attr_trans_A is False) and\n",
    "                (node.matmul_attr_trans_B is True)):\n",
    "            # if Y=A B^T, then dA=dY B^T, dB=(A^T dY)^T=dY^T A\n",
    "            lhs_grad = matmul_op(\n",
    "                output_grad, node.inputs[1], trans_A=False, trans_B=True)\n",
    "            rhs_grad = matmul_op(\n",
    "                output_grad, node.inputs[0], trans_A=True, trans_B=False)\n",
    "        elif ((node.matmul_attr_trans_A is True) and\n",
    "                (node.matmul_attr_trans_B is True)):\n",
    "            # if Y=A^T B^T, then dA=(dY B^T)^T=B dY^T, dB=(A^T dY)^T=dY^T A\n",
    "            lhs_grad = matmul_op(\n",
    "                node.inputs[1], output_grad, trans_A=False, trans_B=True)\n",
    "            rhs_grad = matmul_op(\n",
    "                output_grad, node.inputs[0], trans_A=True, trans_B=False)\n",
    "        return [lhs_grad, rhs_grad]\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        assert input_shapes[0].shape[1] == input_shapes[1].shape[0]\n",
    "        return (input_shapes[0].shape[0], input_shapes[1].shape[1])\n",
    "\n",
    "class PlaceholderOp(Op):\n",
    "    def __call__(self):\n",
    "        \"\"\"Creates a variable node.\"\"\"\n",
    "        new_node = Op.__call__(self)\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        assert False, \"placeholder %s values provided by feed_dict\" % node.name\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        return None\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        assert False, \"placeholder %s shape provided by feed_shape\" % node.name\n",
    "\n",
    "\n",
    "class ZerosLikeOp(Op):\n",
    "    def __call__(self, node_A):\n",
    "        \"\"\"Creates a node that represents np.zeros(node_A.shape).\"\"\"\n",
    "        new_node = Op.__call__(self)\n",
    "        new_node.inputs = [node_A]\n",
    "        new_node.name = \"Zeroslike(%s)\" % node_A.name\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        assert len(input_vals) == 1\n",
    "        if use_numpy:\n",
    "            output_val[:] = np.zeros(input_vals[0].shape)\n",
    "        else:\n",
    "            gpu_op.array_set(output_val, 0)\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        return [zeroslike_op(node.inputs[0])]\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        \"\"\"If input_shape is a vector, simpler to return (1,)\"\"\"\n",
    "        return input_shapes[0]\n",
    "\n",
    "\n",
    "class OnesLikeOp(Op):\n",
    "    def __call__(self, node_A):\n",
    "        \"\"\"Creates a node that represents np.ones(node_A.shape).\"\"\"\n",
    "        new_node = Op.__call__(self)\n",
    "        new_node.inputs = [node_A]\n",
    "        new_node.name = \"Oneslike(%s)\" % node_A.name\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        assert len(input_vals) == 1\n",
    "        if use_numpy:\n",
    "            output_val[:] = np.ones(input_vals[0].shape)\n",
    "        else:\n",
    "            gpu_op.array_set(output_val, 1)\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        return [zeroslike_op(node.inputs[0])]\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        \"\"\"If input_shape is a vector, simpler to return (1,)\"\"\"\n",
    "        return input_shapes[0]\n",
    "\n",
    "\n",
    "class ReduceSumAxisZeroOp(Op):\n",
    "    def __call__(self, node_A):\n",
    "        \"\"\"Creates a node that represents np.sum(node_A, axis=0).\n",
    "        Only support common-case axis=0 reduction for simplicity of gradient.\n",
    "        \"\"\"\n",
    "        new_node = Op.__call__(self)\n",
    "        new_node.inputs = [node_A]\n",
    "        new_node.name = \"ReduceSumAxisZero(%s)\" % (node_A.name)\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        assert len(input_vals) == 1\n",
    "        if use_numpy:\n",
    "            assert(isinstance(input_vals[0], np.ndarray))\n",
    "            output_val[:] = np.sum(input_vals[0], axis=0)\n",
    "        else:\n",
    "            gpu_op.reduce_sum_axis_zero(input_vals[0], output_val)\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        return [broadcastto_op(output_grad, node.inputs[0])]\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        \"\"\"summation reduction axis = 0\n",
    "        e.g. (3,4,5)->(4,5)\n",
    "        for vector, simpler to do (3,)->(1,)\n",
    "        \"\"\"\n",
    "        if len(input_shapes[0]) == 1:\n",
    "            return (1,)\n",
    "        return input_shapes[0][1:]\n",
    "\n",
    "\n",
    "class BroadcastToOp(Op):\n",
    "    def __call__(self, node_A, node_B):\n",
    "        \"\"\"Creates a node that represents np.broadcast_to(node_A, node_B.shape).\n",
    "        Only support axis=0. e.g. (3,4)->(2,3,4) to make gradient simple.\n",
    "        \"\"\"\n",
    "        new_node = Op.__call__(self)\n",
    "        new_node.inputs = [node_A, node_B]\n",
    "        new_node.name = \"BroadcastTo(%s,%s.shape)\" % (node_A.name, node_B.name)\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        assert(len(input_vals)==2)\n",
    "        if use_numpy:\n",
    "            output_val[:] = np.broadcast_to(input_vals[0], input_vals[1].shape)\n",
    "        else:\n",
    "            gpu_op.broadcast_to(input_vals[0], output_val)\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        grad_A = reducesumaxiszero_op(output_grad)\n",
    "        grad_B = zeroslike_op(node.inputs[1])\n",
    "        return [grad_A, grad_B]\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        return broadcast_rule(input_shapes[0], input_shapes[1])\n",
    "\n",
    "\n",
    "def softmax_func(y):\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    b = y - np.max(y, axis=1, keepdims=True)\n",
    "    expb = np.exp(b)\n",
    "    softmax = expb / np.sum(expb, axis=1, keepdims=True)\n",
    "    return softmax\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropyOp(Op):\n",
    "    def __call__(self, node_A, node_B):\n",
    "        new_node = Op.__call__(self)\n",
    "        new_node.inputs = [node_A, node_B]\n",
    "        new_node.name = \"SoftmaxXEntropy(%s,%s)\" % (node_A.name, node_B.name)\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        assert len(input_vals) == 2\n",
    "        y = input_vals[0]\n",
    "        y_ = input_vals[1]\n",
    "        if use_numpy:\n",
    "            softmax = softmax_func(y)\n",
    "            cross_entropy = np.mean(\n",
    "                -np.sum(y_ * np.log(softmax), axis=1), keepdims=True)\n",
    "            output_val[:] = cross_entropy\n",
    "        else:\n",
    "            gpu_op.softmax_cross_entropy(y, y_, output_val)\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        grad_A = (softmax_op(node.inputs[0]) + -1 * node.inputs[1])*output_grad\n",
    "        grad_B = zeroslike_op(node.inputs[1])\n",
    "        return [grad_A, grad_B]\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        return (1,)\n",
    "\n",
    "\n",
    "class SoftmaxOp(Op):\n",
    "    def __call__(self, node_A):\n",
    "        new_node = Op.__call__(self)\n",
    "        new_node.inputs = [node_A]\n",
    "        new_node.name = \"Softmax(%s)\" % (node_A.name)\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        assert len(input_vals) == 1\n",
    "        if use_numpy:\n",
    "            output_val[:] = softmax_func(input_vals[0])\n",
    "        else:\n",
    "            gpu_op.softmax(input_vals[0], output_val)\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        # Do not directly use SoftmaxOp, use SoftmaxCrossEntropyOp instead.\n",
    "        # Not allowing taking 2nd derivative of SoftmaxCrossEntropyOp.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        return input_shapes[0]\n",
    "\n",
    "\n",
    "\n",
    "class ReluOp(Op):\n",
    "    def __call__(self, node_A):\n",
    "        new_node = Op.__call__(self)\n",
    "        new_node.inputs = [node_A]\n",
    "        new_node.name = \"Relu(%s)\" % (node_A.name)\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        assert len(input_vals) == 1\n",
    "        if use_numpy:\n",
    "            output_val[:] = np.maximum(input_vals[0], 0)\n",
    "        else:\n",
    "            gpu_op.relu(input_vals[0], output_val)\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        return [relu_gradient_op(node.inputs[0], output_grad)]\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        return input_shapes[0]\n",
    "\n",
    "\n",
    "class ReluGradientOp(Op):\n",
    "    def __call__(self, node_A, node_B):\n",
    "        \"\"\"node_B is output_grad\"\"\"\n",
    "        new_node = Op.__call__(self)\n",
    "        new_node.inputs = [node_A, node_B]\n",
    "        new_node.name = \"ReluGradient(%s)\" % (node_A.name)\n",
    "        return new_node\n",
    "\n",
    "    def compute(self, node, input_vals, output_val, use_numpy=True):\n",
    "        assert len(input_vals) == 2\n",
    "        if use_numpy:\n",
    "            # heaviside function, 0.5 at x=0\n",
    "            output_val[:] = (np.sign(input_vals[0]) + 1) * 0.5 * input_vals[1]\n",
    "        else:\n",
    "            gpu_op.relu_gradient(input_vals[0], input_vals[1], output_val)\n",
    "\n",
    "    def gradient(self, node, output_grad):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def infer_shape(self, node, input_shapes):\n",
    "        assert input_shapes[0] == input_shapes[1]\n",
    "        return input_shapes[0]\n",
    "\n",
    "\n",
    "\n",
    "# Create global singletons of operators.\n",
    "add_op = AddOp()\n",
    "mul_op = MulOp()\n",
    "add_byconst_op = AddByConstOp()\n",
    "mul_byconst_op = MulByConstOp()\n",
    "matmul_op = MatMulOp()\n",
    "placeholder_op = PlaceholderOp()\n",
    "oneslike_op = OnesLikeOp()\n",
    "zeroslike_op = ZerosLikeOp()\n",
    "reducesumaxiszero_op = ReduceSumAxisZeroOp()\n",
    "broadcastto_op = BroadcastToOp()\n",
    "softmaxcrossentropy_op = SoftmaxCrossEntropyOp()\n",
    "softmax_op = SoftmaxOp()\n",
    "relu_op = ReluOp()\n",
    "relu_gradient_op = ReluGradientOp()\n",
    "\n",
    "\n",
    "class Executor(object):\n",
    "    \"\"\"Executor computes values for given set of nodes in computation graph.\"\"\"\n",
    "    def __init__(self, eval_node_list, ctx=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        eval_node_list: list of nodes whose values need to be computed.\n",
    "        ctx: runtime DLContext, default is None which means np.ndarray on cpu\n",
    "        topo_order: list of nodes in topological order\n",
    "        node_to_shape_map: dict from node to shape of the node\n",
    "        node_to_arr_map: dict from node to ndarray.NDArray allocated for node\n",
    "        feed_shapes: shapes of feed_dict from last run(...)\n",
    "        \"\"\"\n",
    "        self.eval_node_list = eval_node_list\n",
    "        self.ctx = ctx\n",
    "        self.topo_order = find_topo_sort(self.eval_node_list)\n",
    "        self.node_to_shape_map = None\n",
    "        self.node_to_arr_map = None\n",
    "        self.feed_shapes = None\n",
    "\n",
    "    def infer_shape(self, feed_shapes):\n",
    "        \"\"\"Given shapes of feed_dict nodes, infer shape for all nodes in graph.\n",
    "\n",
    "        Implementation note:\n",
    "        Iteratively calls node.op.infer_shape to infer shapes.\n",
    "        Node shapes stored in self.node_to_shape_map.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feed_shapes: node->shapes mapping for feed_dict nodes.\n",
    "        \"\"\"\n",
    "        node_to_shape_map = {}\n",
    "        for node, shape in feed_shapes.items():\n",
    "            node_to_shape_map[node] = shape\n",
    "\n",
    "        for node in self.topo_order:\n",
    "            if node in node_to_shape_map:\n",
    "                continue\n",
    "            input_shades = [node_to_shape_map[n] for n in node.inputs]\n",
    "            node_to_shape_map[node] = node.op.infer_shape(node, input_shades)\n",
    "\n",
    "        self.node_to_shape_map = node_to_shape_map\n",
    "\n",
    "\n",
    "\n",
    "    def memory_plan(self, feed_shapes):\n",
    "        \"\"\"Allocates ndarray.NDArray for every node except feed_dict nodes.\n",
    "\n",
    "        Implementation note:\n",
    "        Option 1: Alloc a ndarray.NDArray per node that persists across run()\n",
    "        Option 2: Implement a memory pool to reuse memory for nodes of same\n",
    "                shapes. More details see Lecture 7.\n",
    "\n",
    "        For both options, self.node_to_arr_map stores node->NDArray mapping to\n",
    "        allow mapping to persist across multiple executor.run().\n",
    "\n",
    "        Hint: use ndarray.empty(shape, ctx=self.ctx) to allocate NDArray.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feed_shapes: node->shapes mapping for feed_dict nodes.\n",
    "        \"\"\"\n",
    "\n",
    "        shape_to_ndarray = {}\n",
    "        node_to_arr_map = {}\n",
    "        \n",
    "        for node, shape in self.node_to_shape_map:\n",
    "            if node not in feed_shapes:\n",
    "                shape_to_ndarray[shape] = []\n",
    "\n",
    "        def get_mem(shape):\n",
    "            if len(shape_to_ndarray[shape]) > 0:\n",
    "                return shape_to_ndarray[shape].pop()\n",
    "            return ndarray.empty(shape, ctx=self.ctx)\n",
    "\n",
    "        def free_mem(arr):\n",
    "            if arr is not None:\n",
    "                shape_to_ndarray[arr.shape].append(arr)\n",
    "        \n",
    "        out_deg = {}\n",
    "        for node in self.topo_order:\n",
    "            out_deg[node] = 0\n",
    "        for node in self.topo_order:\n",
    "            for input_node in node.inputs:\n",
    "                out_deg[input_node] += 1\n",
    "\n",
    "        for node in self.topo_order:\n",
    "            if node in feed_shapes:\n",
    "                continue\n",
    "\n",
    "            node_to_arr_map[node] = get_mem(node.shape)\n",
    "\n",
    "            for input_node in node.inputs:\n",
    "                out_deg[input_node] -= 1\n",
    "                if out_deg[input_node] == 0:\n",
    "                    if input_node in self.eval_node_list:\n",
    "                        continue\n",
    "                    free_mem(node_to_arr_map.get(input_node))\n",
    "\n",
    "        self.node_to_arr_map = node_to_arr_map\n",
    "\n",
    "\n",
    "    def run(self, feed_dict, convert_to_numpy_ret_vals=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        feed_dict: a dictionary of node->np.ndarray supplied by user.\n",
    "        convert_to_numpy_ret_vals: whether to convert ret vals to np.array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A list of values for nodes in eval_node_list. NDArray or np.ndarray.\n",
    "        \"\"\"\n",
    "        def are_feed_shapes_equal(sa, sb):\n",
    "            if (not isinstance(sa, dict)) or (not isinstance(sb, dict)):\n",
    "                return False\n",
    "            unmatched_item = set(sa.items()) ^ set(sb.items())\n",
    "            return len(unmatched_item) == 0\n",
    "\n",
    "        # Assume self.ctx is None implies numpy array and numpy ops.\n",
    "        use_numpy = self.ctx is None\n",
    "        node_to_val_map = {}\n",
    "        for node, value in feed_dict.items():\n",
    "            if use_numpy:\n",
    "                # all values passed in feed_dict must be np.ndarray\n",
    "                assert isinstance(value, np.ndarray)\n",
    "                node_to_val_map[node] = value\n",
    "            else:\n",
    "                # convert values to ndarray.NDArray if necessary\n",
    "                if isinstance(value, np.ndarray):\n",
    "                    node_to_val_map[node] = ndarray.array(value, ctx=self.ctx)\n",
    "                elif isinstance(value, ndarray.NDArray):\n",
    "                    node_to_val_map[node] = value\n",
    "                else:\n",
    "                    assert False, \"feed_dict value type not supported\"\n",
    "\n",
    "        # collect shapes for all placeholders\n",
    "        feed_shapes = {}\n",
    "        for node in node_to_val_map:\n",
    "            feed_shapes[node] = node_to_val_map[node].shape\n",
    "\n",
    "        # infer shape if feed_shapes changed since last run\n",
    "        # e.g. call run() on test data after trainng\n",
    "        if (not are_feed_shapes_equal(feed_shapes, self.feed_shapes)):\n",
    "            self.infer_shape(feed_shapes)\n",
    "            self.feed_shapes = feed_shapes\n",
    "            # plan memory if using GPU\n",
    "            if (not use_numpy):\n",
    "                self.memory_plan(feed_shapes)\n",
    "\n",
    "        # Traverse graph in topo order and compute values for all nodes.\n",
    "        for node in self.topo_order:\n",
    "            if node in node_to_val_map:\n",
    "                # Skip placeholder nodes. Values already provided by feed_dict.\n",
    "                continue\n",
    "            input_vals = [node_to_val_map[n] for n in node.inputs]\n",
    "            if use_numpy:\n",
    "                node_val = np.empty(shape=self.node_to_shape_map[node])\n",
    "            else:\n",
    "                node_val = self.node_to_arr_map[node]\n",
    "            # node_val is modified in-place whether np.ndarray or NDArray\n",
    "            node.op.compute(node, input_vals, node_val, use_numpy)\n",
    "            node_to_val_map[node] = node_val\n",
    "\n",
    "        # Collect node values.\n",
    "        if not use_numpy and convert_to_numpy_ret_vals:\n",
    "            return [node_to_val_map[n].asnumpy() for n in self.eval_node_list]\n",
    "        return [node_to_val_map[n] for n in self.eval_node_list]\n",
    "\n",
    "\n",
    "def gradients(output_node, node_list):\n",
    "    \"\"\"Take gradient of output node with respect to each node in node_list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_node: output node that we are taking derivative of.\n",
    "    node_list: list of nodes that we are taking derivative wrt.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A list of gradient values, one for each node in node_list respectively.\n",
    "\n",
    "    \"\"\"\n",
    "    node_to_output_grads_list = {}\n",
    "    node_to_output_grads_list[output_node] = [oneslike_op(output_node)]\n",
    "    node_to_output_grad = {}\n",
    "    # Traverse forward graph in reverse topological order\n",
    "    reverse_topo_order = reversed(find_topo_sort([output_node]))\n",
    "    for node in reverse_topo_order:\n",
    "        output_grad = sum_node_list(node_to_output_grads_list[node])\n",
    "        node_to_output_grad[node] = output_grad\n",
    "        input_grads_list = node.op.gradient(node, output_grad)\n",
    "        for i in range(len(node.inputs)):\n",
    "            if node.inputs[i] not in node_to_output_grads_list:\n",
    "                node_to_output_grads_list[node.inputs[i]] = []\n",
    "            # Calculate partial adjoint for input nodes.\n",
    "            node_to_output_grads_list[node.inputs[i]].append(\n",
    "                input_grads_list[i])\n",
    "\n",
    "    grad_node_list = [node_to_output_grad[node] for node in node_list]\n",
    "    return grad_node_list\n",
    "\n",
    "##################\n",
    "# Helper Methods #\n",
    "##################\n",
    "\n",
    "\n",
    "def find_topo_sort(node_list):\n",
    "    \"\"\"Given a list of nodes, return a topo ordering of nodes ending in them.\n",
    "\n",
    "    A simple algorithm is to do a post-order DFS traversal on the given nodes,\n",
    "    going backwards based on input edges. Since a node is added to the ordering\n",
    "    after all its predecessors are traversed due to post-order DFS, we get a\n",
    "    topological sort.\n",
    "\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    topo_order = []\n",
    "    for node in node_list:\n",
    "        topo_sort_dfs(node, visited, topo_order)\n",
    "    return topo_order\n",
    "\n",
    "\n",
    "def topo_sort_dfs(node, visited, topo_order):\n",
    "    \"\"\"Post-order DFS\"\"\"\n",
    "    if node in visited:\n",
    "        return\n",
    "    visited.add(node)\n",
    "    for n in node.inputs:\n",
    "        topo_sort_dfs(n, visited, topo_order)\n",
    "    topo_order.append(node)\n",
    "\n",
    "\n",
    "def sum_node_list(node_list):\n",
    "    \"\"\"Custom sum func to avoid creating redundant nodes in Python sum func.\"\"\"\n",
    "    from operator import add\n",
    "    from functools import reduce\n",
    "    return reduce(add, node_list)\n",
    "\n",
    "\n",
    "def broadcast_rule(shape_a, shape_b):\n",
    "    \"\"\"Return output shape of broadcast shape_a, shape_b.\n",
    "    e.g. broadcast_rule((3,2), (4,3,2))\n",
    "    returns output_shape = (4,3,2)\n",
    "\n",
    "    Check out explanations and more examples at\n",
    "    https://docs.scipy.org/doc/numpy-1.10.0/user/basics.broadcasting.html\n",
    "    http://eli.thegreenplace.net/2015/broadcasting-arrays-in-numpy/\n",
    "    \"\"\"\n",
    "    assert(isinstance(shape_a, tuple))\n",
    "    assert(isinstance(shape_b, tuple))\n",
    "    if len(shape_a) > len(shape_b):\n",
    "        longer_shape, shorter_shape = shape_a, shape_b\n",
    "    else:\n",
    "        longer_shape, shorter_shape = shape_b, shape_a\n",
    "    len_diff = len(longer_shape) - len(shorter_shape)\n",
    "    for i in range(len_diff):\n",
    "        # pad with leading 1s\n",
    "        shorter_shape = (1,) + shorter_shape\n",
    "    assert len(shorter_shape) == len(longer_shape)\n",
    "    output_shape = list(longer_shape)\n",
    "    for i in range(len(output_shape)):\n",
    "        assert (shorter_shape[i] == longer_shape[i]) \\\n",
    "            or (shorter_shape[i] == 1) \\\n",
    "            or (longer_shape[i] == 1)\n",
    "        output_shape[i] = max(shorter_shape[i], longer_shape[i])\n",
    "    return tuple(output_shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
